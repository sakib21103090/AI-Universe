{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmE0FfGwX7RR+UAo5BXXhE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakib21103090/AI-Universe/blob/main/AQI_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing**"
      ],
      "metadata": {
        "id": "-enrjw2YIh4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/Narsingdi data/NARSINGDI 21-23 Data.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"Dataset Preview:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Separate numeric and non-numeric columns\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "# Impute missing values in numeric columns using the mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_numeric_imputed = pd.DataFrame(imputer.fit_transform(df[numeric_columns]), columns=numeric_columns)\n",
        "\n",
        "# For non-numeric columns, we can use a different strategy (e.g., 'most_frequent')\n",
        "imputer_non_numeric = SimpleImputer(strategy='most_frequent')\n",
        "df_non_numeric_imputed = pd.DataFrame(imputer_non_numeric.fit_transform(df[non_numeric_columns]), columns=non_numeric_columns)\n",
        "\n",
        "# Combine numeric and non-numeric columns back together\n",
        "df_imputed = pd.concat([df_numeric_imputed, df_non_numeric_imputed], axis=1)\n",
        "\n",
        "# Confirm that missing values are handled\n",
        "print(\"\\nAfter Imputation, Missing Values per Column:\")\n",
        "print(df_imputed.isnull().sum())\n",
        "\n",
        "# Save the cleaned dataset to a new CSV file\n",
        "df_imputed.to_csv('/content/Narsingdi data/Narsingdi 21-23Data_cleaned.csv', index=False)\n",
        "\n",
        "print(\"\\nCleaned dataset saved as 'Narsingdi 21-23Data_cleaned.csv'.\")\n"
      ],
      "metadata": {
        "id": "K-4npHwoIoaA",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4caa820c-f70d-4c08-d1e8-0f6b9f87f26e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Preview:\n",
            "          Date     SO2     NO     NO2    NOX    CO     O3   PM10   PM25\n",
            "0  1/1/2021 1:00   0.33   3.65  13.55  17.20  1.64  42.49    NaN   52.5\n",
            "1  1/1/2021 2:00   0.34   5.22  12.73  17.95  1.29  32.12    NaN   58.4\n",
            "2  1/1/2021 3:00   0.33   2.11   8.85  10.96  1.11  41.77    NaN   52.1\n",
            "3  1/1/2021 4:00   0.38   6.69  10.26  16.95  1.11  36.44    NaN   63.0\n",
            "4  1/1/2021 5:00   0.40  12.00  12.94  24.94  0.86  31.99    NaN   83.8\n",
            "\n",
            "Missing Values per Column:\n",
            " Date         0\n",
            " SO2       2993\n",
            " NO        9089\n",
            " NO2       6502\n",
            " NOX       6453\n",
            " CO        4189\n",
            " O3        3129\n",
            " PM10      4174\n",
            " PM25      3198\n",
            "dtype: int64\n",
            "\n",
            "After Imputation, Missing Values per Column:\n",
            " SO2       0\n",
            " NO        0\n",
            " NO2       0\n",
            " NOX       0\n",
            " CO        0\n",
            " O3        0\n",
            " PM10      0\n",
            " PM25      0\n",
            " Date      0\n",
            "dtype: int64\n",
            "\n",
            "Cleaned dataset saved as 'Narsingdi 21-23Data_cleaned.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AQI Calculation**"
      ],
      "metadata": {
        "id": "NV2eNBuWDKt2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kPDpljbeXC04"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the function to calculate AQI for a given pollutant\n",
        "def calculate_aqi(C, C_low, C_high, I_low, I_high):\n",
        "    if C_low <= C <= C_high:\n",
        "        aqi = ((I_high - I_low) / (C_high - C_low)) * (C - C_low) + I_low\n",
        "        return round(aqi, 2)\n",
        "    else:\n",
        "        return None  # If C is outside the defined range\n",
        "\n",
        "# Breakpoints for AQI calculation for different pollutants\n",
        "aqi_breakpoints = {\n",
        "\n",
        "    \"SO2\": [(0, 30, 0, 50), (31, 60, 51, 100),\n",
        "            (61, 90, 101, 200), (91, 120, 201, 300),\n",
        "            (121, 250, 301, 400), (251, 350, 401, 500)],\n",
        "\n",
        "    \"NO \": [(0, 50, 0, 50), (51, 100, 51, 100),\n",
        "             (101, 250, 101, 200), (251, 350, 201, 300),\n",
        "             (351, 430, 301, 400), (431, 500, 401, 500)],\n",
        "    \"NO2\": [(0, 40, 0, 50), (41, 80, 51, 100),\n",
        "             (81, 380, 101, 200), (381, 800, 201, 300),\n",
        "            (801, 1600, 301, 400), (1601, 2100, 401, 500)],\n",
        "\n",
        "    \"NOX\": [(0, 1, 0, 50), (2, 10, 51, 100),\n",
        "            (11, 17, 101, 200), (18, 34, 201, 300),\n",
        "           (35, 50, 301, 400), (51, 60, 401, 500)],\n",
        "\n",
        "    \"CO\": [(0, 50, 0, 50), (51, 100, 51, 100),\n",
        "           (101, 168, 101, 200), (169, 208, 201, 300),\n",
        "           (209, 748, 301, 400), (749, 1000, 401, 500)],\n",
        "\n",
        "    \"O3\": [(0, 50, 0, 50), (51, 100, 51, 100),\n",
        "           (101, 168, 101, 200), (169, 208, 201, 300),\n",
        "           (209, 748, 301, 400), (749, 1000, 401, 500)],\n",
        "\n",
        "    \"PM10\": [(0, 40, 0, 50), (41, 80, 51, 100),\n",
        "           (81, 180, 101, 200), (181, 280, 201, 300),\n",
        "           (281, 400, 301, 400), (401, 500, 401, 500)],\n",
        "    \"PM25\": [(0, 40, 0, 50), (41, 80, 51, 100),\n",
        "           (81, 180, 101, 200), (181, 280, 201, 300),\n",
        "           (281, 400, 301, 400), (401, 500, 401, 500)]\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the dataset from a CSV file and clean the column names\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    # Strip any leading or trailing spaces from column names\n",
        "    data.columns = data.columns.str.strip()\n",
        "    print(\"Column names in dataset:\", data.columns)  # Print column names for debugging\n",
        "    return data\n",
        "\n",
        "# Function to find the AQI breakpoints based on pollutant concentration\n",
        "def find_breakpoints(pollutant, C):\n",
        "    if pollutant in aqi_breakpoints:\n",
        "        for bp in aqi_breakpoints[pollutant]:\n",
        "            C_low, C_high, I_low, I_high = bp\n",
        "            if C_low <= C <= C_high:\n",
        "                return C_low, C_high, I_low, I_high\n",
        "    return None, None, None, None"
      ],
      "metadata": {
        "id": "j3CsdxeKgNQS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate AQI for each pollutant in the dataset\n",
        "def calculate_aqi_for_dataset(data):\n",
        "    pollutants = [ 'SO2','NO','NO2','NOX','CO','O3','PM25', 'PM10']  # Only pollutants with AQI breakpoints\n",
        "\n",
        "    for pollutant in pollutants:\n",
        "        aqi_column = pollutant + '_AQI'\n",
        "        data[aqi_column] = None  # Create a new column for AQI for each pollutant\n",
        "\n",
        "        for index, row in data.iterrows():\n",
        "            if pollutant in row and not pd.isna(row[pollutant]):\n",
        "                concentration = row[pollutant]\n",
        "                C_low, C_high, I_low, I_high = find_breakpoints(pollutant, concentration)\n",
        "\n",
        "                if C_low is not None:\n",
        "                    aqi = calculate_aqi(concentration, C_low, C_high, I_low, I_high)\n",
        "                    data.at[index, aqi_column] = aqi\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "nxKJcjTAgeNS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the highest AQI for each row\n",
        "def calculate_highest_aqi(data):\n",
        "    pollutants = [ 'SO2_AQI','NO_AQI','NO2_AQI','NOX_AQI','CO_AQI','O3_AQI','PM25_AQI', 'PM10_AQI']\n",
        "\n",
        "    # Create a new column 'AQI' that holds the highest AQI value among the pollutants\n",
        "    data['AQI'] = data[pollutants].max(axis=1)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "jb-W_3K9gj_t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to load the dataset, calculate the AQI, and save results\n",
        "def main():\n",
        "    # Replace with your actual file path\n",
        "    file_path = '/content/Narsingdi data/Narsingdi 21-23Data_cleaned.csv'\n",
        "    data = load_data(file_path)\n",
        "\n",
        "    # Calculate AQI for each pollutant\n",
        "    aqi_data = calculate_aqi_for_dataset(data)\n",
        "\n",
        "    # Calculate the highest AQI and add to the 'AQI' column\n",
        "    aqi_data = calculate_highest_aqi(aqi_data)\n",
        "\n",
        "    # Reorder columns: pollutants on the left, AQI values on the right\n",
        "    pollutants = ['SO2', 'NO', 'NO2', 'NOX', 'CO', 'O3', 'PM25', 'PM10']  # Corrected the typo by adding a comma\n",
        "    aqi_columns = [pollutant + '_AQI' for pollutant in pollutants]\n",
        "    columns_to_save = ['Date'] + pollutants + aqi_columns + ['AQI']  # 'Date', pollutants, AQI columns, and 'AQI'\n",
        "\n",
        "    # Filter and reorder columns\n",
        "    filtered_data = aqi_data[columns_to_save]\n",
        "\n",
        "    # Save the updated dataset to a new CSV file\n",
        "    filtered_data.to_csv('/content/Narsingdi data/Narsingdi 21-23Data_cleaned_With_AQI.csv', index=False)\n",
        "    print('CSV file saved successfully with pollutants, AQI values, and the highest AQI.')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "CB_stlw1gnNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "425c7c50-479e-42dc-8f8f-fb8a3038de4a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in dataset: Index(['SO2', 'NO', 'NO2', 'NOX', 'CO', 'O3', 'PM10', 'PM25', 'Date'], dtype='object')\n",
            "CSV file saved successfully with pollutants, AQI values, and the highest AQI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AQI Buckets**"
      ],
      "metadata": {
        "id": "X1Yl07PYDX60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wOD7c5CtfCfh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/Narsingdi data/Narsingdi 21-23Data_cleaned_With_AQI.csv')"
      ],
      "metadata": {
        "id": "592e1rk6jArm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to assign AQI buckets based on AQI values\n",
        "def assign_aqi_bucket(aqi):\n",
        "    if aqi <= 50:\n",
        "        return 'Good'\n",
        "    elif aqi <= 100:\n",
        "        return 'Moderate'\n",
        "    elif aqi <= 150:\n",
        "        return 'Unhealthy for Sensitive Groups'\n",
        "    elif aqi <= 200:\n",
        "        return 'Unhealthy'\n",
        "    elif aqi <= 300:\n",
        "        return 'Very Unhealthy'\n",
        "    else:\n",
        "        return 'Hazardous'\n",
        "\n",
        "# Apply the function to create a new column 'AQI_Bucket'\n",
        "df['AQI_Bucket'] = df['AQI'].apply(assign_aqi_bucket)\n",
        "\n",
        "# Step 2: Save the updated dataset to a new CSV file\n",
        "df.to_csv('/content/Narsingdi data/Narsingdi 21-23Data_cleaned_With_AQI_Bucket.csv', index=False)\n",
        "print('CSV file saved successfully.')\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "zVNyJceGfJLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1faf73c2-9d14-4f75-ea92-5ee16dae3c8b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file saved successfully.\n",
            "            Date   SO2     NO    NO2    NOX    CO     O3  PM25        PM10  \\\n",
            "0  1/1/2021 1:00  0.33   3.65  13.55  17.20  1.64  42.49  52.5  110.676109   \n",
            "1  1/1/2021 2:00  0.34   5.22  12.73  17.95  1.29  32.12  58.4  110.676109   \n",
            "2  1/1/2021 3:00  0.33   2.11   8.85  10.96  1.11  41.77  52.1  110.676109   \n",
            "3  1/1/2021 4:00  0.38   6.69  10.26  16.95  1.11  36.44  63.0  110.676109   \n",
            "4  1/1/2021 5:00  0.40  12.00  12.94  24.94  0.86  31.99  83.8  110.676109   \n",
            "\n",
            "   SO2_AQI  NO_AQI  NO2_AQI  NOX_AQI  CO_AQI  O3_AQI  PM25_AQI  PM10_AQI  \\\n",
            "0     0.55     NaN    16.94      NaN    1.64   42.49     65.45    130.68   \n",
            "1     0.57     NaN    15.91      NaN    1.29   32.12     72.86    130.68   \n",
            "2     0.55     NaN    11.06      NaN    1.11   41.77     64.95    130.68   \n",
            "3     0.63     NaN    12.82   199.17    1.11   36.44     78.64    130.68   \n",
            "4     0.67     NaN    16.18   243.94    0.86   31.99    103.80    130.68   \n",
            "\n",
            "      AQI                      AQI_Bucket  \n",
            "0  130.68  Unhealthy for Sensitive Groups  \n",
            "1  130.68  Unhealthy for Sensitive Groups  \n",
            "2  130.68  Unhealthy for Sensitive Groups  \n",
            "3  199.17                       Unhealthy  \n",
            "4  243.94                  Very Unhealthy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression Algorithm**"
      ],
      "metadata": {
        "id": "HyLarqUaNGGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "zrwSEw5qNGGY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load your dataset\n",
        "file_path = '/content/Narsingdi data/Narsingdi 21-23Data_cleaned_With_AQI_Bucket.csv'  # Replace with your actual file path\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "AenKYFKDNGGY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Data preprocessing and feature selection\n",
        "# Selecting relevant columns\n",
        "X = df[['SO2', 'NO', 'NO2', 'NOX', 'CO', 'O3', 'PM25', 'PM10']]\n",
        "y = df['AQI_Bucket']  # Assuming AQI_Bucket is categorical"
      ],
      "metadata": {
        "id": "Af44DygzNGGZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Splitting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "t6nwp7rENGGZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4: Feature scaling (if necessary)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "GCSpqUrxNGGZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Training the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "collapsed": true,
        "outputId": "2879ba84-4216-4f2c-f423-303df356ed5e",
        "id": "0ZpiS_dZNGGZ"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Predicting on the test set\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "8JQ-qDBBNGGa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Evaluating the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)  # Added classification_report"
      ],
      "metadata": {
        "id": "-1LUYET0NGGa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "print(\"Classification Report:\\n\", class_report)\n",
        "\n",
        "# Step 8: Print feature importance\n",
        "features = X.columns  # Assuming X is a DataFrame with named columns\n",
        "feature_importance = pd.DataFrame({'feature': features, 'importance': abs(model.coef_[0])})\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance.sort_values('importance', ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded49257-da08-4cee-a951-27e6039b861a",
        "id": "B8_KtQ8FNGGa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6774193548387096\n",
            "Confusion Matrix:\n",
            "[[  96    0   11    0   28    0]\n",
            " [   0  160    0    4   14  196]\n",
            " [  13    3  728    0  158    6]\n",
            " [   0    8    9   14  499   32]\n",
            " [   0    6  127    8 1643    4]\n",
            " [   1   27    0   38  248  383]]\n",
            "Classification Report:\n",
            "                                 precision    recall  f1-score   support\n",
            "\n",
            "                          Good       0.87      0.71      0.78       135\n",
            "                     Hazardous       0.78      0.43      0.55       374\n",
            "                      Moderate       0.83      0.80      0.82       908\n",
            "                     Unhealthy       0.22      0.02      0.04       562\n",
            "Unhealthy for Sensitive Groups       0.63      0.92      0.75      1788\n",
            "                Very Unhealthy       0.62      0.55      0.58       697\n",
            "\n",
            "                      accuracy                           0.68      4464\n",
            "                     macro avg       0.66      0.57      0.59      4464\n",
            "                  weighted avg       0.64      0.68      0.63      4464\n",
            "\n",
            "\n",
            "Feature Importance:\n",
            "  feature  importance\n",
            "7    PM10    9.082490\n",
            "6    PM25    6.014205\n",
            "2     NO2    3.969566\n",
            "1      NO    2.084683\n",
            "0     SO2    1.907680\n",
            "3     NOX    0.777887\n",
            "5      O3    0.120748\n",
            "4      CO    0.061840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uWN_ZfLp_H7d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PsjCz2aECayx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZE6WPnetDSkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Decision Tree Classifier**\n"
      ],
      "metadata": {
        "id": "8uA6yn1vFlZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Step 1: Load dataset\n",
        "df = pd.read_csv('/content/Narsingdi data/Narsingdi 21-23Data_cleaned_With_AQI_Bucket.csv')\n",
        "\n",
        "# Step 2: Preprocessing\n",
        "X = df[['SO2', 'NO', 'NO2', 'NOX', 'CO', 'O3', 'PM25', 'PM10']]\n",
        "y = df['AQI_Bucket']\n",
        "\n",
        "# Step 3: Train-Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Decision Tree model\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oji31BTYFn36",
        "outputId": "aa307278-283f-4e26-a7c0-2b0379d8c616"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9939516129032258\n",
            "Confusion Matrix:\n",
            " [[ 130    0    5    0    0    0]\n",
            " [   0  371    0    1    2    0]\n",
            " [   3    1  903    0    1    0]\n",
            " [   0    0    1  558    3    0]\n",
            " [   0    0    0    1 1787    0]\n",
            " [   0    3    2    3    1  688]]\n",
            "Classification Report:\n",
            "                                 precision    recall  f1-score   support\n",
            "\n",
            "                          Good       0.98      0.96      0.97       135\n",
            "                     Hazardous       0.99      0.99      0.99       374\n",
            "                      Moderate       0.99      0.99      0.99       908\n",
            "                     Unhealthy       0.99      0.99      0.99       562\n",
            "Unhealthy for Sensitive Groups       1.00      1.00      1.00      1788\n",
            "                Very Unhealthy       1.00      0.99      0.99       697\n",
            "\n",
            "                      accuracy                           0.99      4464\n",
            "                     macro avg       0.99      0.99      0.99      4464\n",
            "                  weighted avg       0.99      0.99      0.99      4464\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest Classification**\n"
      ],
      "metadata": {
        "id": "ghkmD99PJYIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/Narsingdi data/Narsingdi 21-23Data_cleaned_With_AQI_Bucket.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Prepare the features and target\n",
        "X = df[['SO2', 'NO', 'NO2', 'NOX', 'CO', 'O3', 'PM25', 'PM10']]\n",
        "y = df['AQI_Bucket']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "rf_pred = rf_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Results:\")\n",
        "print(f'Accuracy: {accuracy_score(y_test, rf_pred)}')\n",
        "print(f'Confusion Matrix:\\n{confusion_matrix(y_test, rf_pred)}')\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, rf_pred))\n",
        "\n",
        "# Print feature importance\n",
        "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': rf_model.feature_importances_})\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance.sort_values('importance', ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7PD9ka-JbEr",
        "outputId": "a27be792-b823-40ef-fb6c-2dcef9bfbed7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Results:\n",
            "Accuracy: 0.9899193548387096\n",
            "Confusion Matrix:\n",
            "[[ 128    0    6    0    1    0]\n",
            " [   0  371    0    1    1    1]\n",
            " [   0    5  893    2    1    7]\n",
            " [   0    5    0  554    2    1]\n",
            " [   0    3    2    1 1779    3]\n",
            " [   0    3    0    0    0  694]]\n",
            "Classification Report:\n",
            "                                 precision    recall  f1-score   support\n",
            "\n",
            "                          Good       1.00      0.95      0.97       135\n",
            "                     Hazardous       0.96      0.99      0.98       374\n",
            "                      Moderate       0.99      0.98      0.99       908\n",
            "                     Unhealthy       0.99      0.99      0.99       562\n",
            "Unhealthy for Sensitive Groups       1.00      0.99      1.00      1788\n",
            "                Very Unhealthy       0.98      1.00      0.99       697\n",
            "\n",
            "                      accuracy                           0.99      4464\n",
            "                     macro avg       0.99      0.98      0.99      4464\n",
            "                  weighted avg       0.99      0.99      0.99      4464\n",
            "\n",
            "\n",
            "Feature Importance:\n",
            "  feature  importance\n",
            "7    PM10    0.346552\n",
            "6    PM25    0.264755\n",
            "3     NOX    0.105991\n",
            "2     NO2    0.075894\n",
            "1      NO    0.060987\n",
            "0     SO2    0.057769\n",
            "4      CO    0.053060\n",
            "5      O3    0.034993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Support Vector Machine Classification**"
      ],
      "metadata": {
        "id": "8X9xV_uuJo5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/Narsingdi data/Narsingdi 21-23Data_cleaned_With_AQI_Bucket.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Prepare the features and target\n",
        "X = df[['SO2', 'NO', 'NO2', 'NOX', 'CO', 'O3', 'PM25', 'PM10']]\n",
        "y = df['AQI_Bucket']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the model\n",
        "svm_model = SVC(random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "svm_pred = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Support Vector Machine Results:\")\n",
        "print(f'Accuracy: {accuracy_score(y_test, svm_pred)}')\n",
        "print(f'Confusion Matrix:\\n{confusion_matrix(y_test, svm_pred)}')\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, svm_pred))\n",
        "\n",
        "# Note: SVM doesn't have built-in feature importance\n",
        "print(\"\\nNote: SVM doesn't provide built-in feature importance.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgCddywMJzf2",
        "outputId": "23d1b8b2-c04c-438e-8d7e-3f62b9a687ad"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Vector Machine Results:\n",
            "Accuracy: 0.8828405017921147\n",
            "Confusion Matrix:\n",
            "[[  97    1   11   25    1    0]\n",
            " [   0  344    0    0    1   29]\n",
            " [  10    7  805   25   58    3]\n",
            " [   0    7    0  407  108   40]\n",
            " [   0    3   48   43 1691    3]\n",
            " [   0   29    0   66    5  597]]\n",
            "Classification Report:\n",
            "                                 precision    recall  f1-score   support\n",
            "\n",
            "                          Good       0.91      0.72      0.80       135\n",
            "                     Hazardous       0.88      0.92      0.90       374\n",
            "                      Moderate       0.93      0.89      0.91       908\n",
            "                     Unhealthy       0.72      0.72      0.72       562\n",
            "Unhealthy for Sensitive Groups       0.91      0.95      0.93      1788\n",
            "                Very Unhealthy       0.89      0.86      0.87       697\n",
            "\n",
            "                      accuracy                           0.88      4464\n",
            "                     macro avg       0.87      0.84      0.85      4464\n",
            "                  weighted avg       0.88      0.88      0.88      4464\n",
            "\n",
            "\n",
            "Note: SVM doesn't provide built-in feature importance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **K-Nearest Neighbors Classification**"
      ],
      "metadata": {
        "id": "k4pJvIfrMS2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/Narsingdi data/Narsingdi 21-23Data_cleaned_With_AQI_Bucket.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Prepare the features and target\n",
        "X = df[['SO2', 'NO', 'NO2', 'NOX', 'CO', 'O3', 'PM25', 'PM10']]\n",
        "y = df['AQI_Bucket']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the model\n",
        "knn_model = KNeighborsClassifier()\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "knn_pred = knn_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"K-Nearest Neighbors Results:\")\n",
        "print(f'Accuracy: {accuracy_score(y_test, knn_pred)}')\n",
        "print(f'Confusion Matrix:\\n{confusion_matrix(y_test, knn_pred)}')\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, knn_pred))\n",
        "\n",
        "# Note: KNN doesn't have built-in feature importance\n",
        "print(\"\\nNote: KNN doesn't provide built-in feature importance.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8I-zrRPMWWF",
        "outputId": "c0df1e58-30dd-4227-c2d6-003d000007a0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-Nearest Neighbors Results:\n",
            "Accuracy: 0.8902329749103942\n",
            "Confusion Matrix:\n",
            "[[ 116    0   12    6    0    1]\n",
            " [   0  334    0    0    3   37]\n",
            " [  11    3  839   10   40    5]\n",
            " [   7    9    7  427   83   29]\n",
            " [   0    3   37   50 1693    5]\n",
            " [  10   47    8   59    8  565]]\n",
            "Classification Report:\n",
            "                                 precision    recall  f1-score   support\n",
            "\n",
            "                          Good       0.81      0.86      0.83       135\n",
            "                     Hazardous       0.84      0.89      0.87       374\n",
            "                      Moderate       0.93      0.92      0.93       908\n",
            "                     Unhealthy       0.77      0.76      0.77       562\n",
            "Unhealthy for Sensitive Groups       0.93      0.95      0.94      1788\n",
            "                Very Unhealthy       0.88      0.81      0.84       697\n",
            "\n",
            "                      accuracy                           0.89      4464\n",
            "                     macro avg       0.86      0.87      0.86      4464\n",
            "                  weighted avg       0.89      0.89      0.89      4464\n",
            "\n",
            "\n",
            "Note: KNN doesn't provide built-in feature importance.\n"
          ]
        }
      ]
    }
  ]
}